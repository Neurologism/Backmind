# LeNet-5

## 1

Have you ever felt confident about a test, only to realize you don’t know the answer to the first question?
Neural networks can experience something similar. They might perform well during training but struggle in real-world scenarios.

## 2

In this tutorial, we’ll learn how to test your neural network to ensure it performs well on new, unseen data.
Testing is essential to ensure your network doesn’t just memorize the training data but generalizes to real-world situations.

## 3

To test a neural network, we need data it hasn’t seen before.
Before training, we typically split the dataset into two parts: one for training and one for testing.

## 4

To split the wine quality dataset, we’ll use the split node. You can find it in the dataset section.

## 5

Next, we need to connect our dataset to the split node.
Start by removing the connection between the dataset and the fit node. Then, connect the dataset to the input of the split node.

## 6

The split node has two outputs: one for each part of the data.
Connect the first output of the split node to the fit node. This will be the training data.

## 7

We also need to specify how much data should go to training and testing.
In the ratio setting of the split node, set the value to 0.8.

## 8

Now, the fit node will receive 80% of the data for training. The remaining 20% will automatically be reserved for testing.

## 9

How do we test the model? We need another node called the evaluate node.
This node will measure how well the model performs on the test data. You can find the evaluate node in the model section.

## 10

Connect the split2 output of the split node to the evaluate node. Also connect the fit node to the evaluate node.

## 11

Great! We’re ready to go. When you hit the start training button, the model will be trained on 80% of the data as usual.
After training, the evaluate node will test the model on the remaining 20%. Go ahead and start training!

## 12

Once training is done, check the evaluate node for results. You’ll see that the Mean Absolute Error (MAE) on the test data
is around 0.5, while the MAE on the training data is about 0.3. This gap indicates that the model isn’t performing as well on test data.

## 13

This problem is called overfitting. It happens when the model learns the training data too well but fails to generalize to new data.
Don’t worry—overfitting is common in neural networks. There are many ways to address it, and we’ll explore one of them in the next tutorial. Stay tuned!
