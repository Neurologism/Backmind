# LeNet-5

## 1

Welcome to round two! In the previous session, we laid the groundwork for your neural network. Now, we’ll take a closer
look at the settings of the layers and dive into the MNIST dataset.

## 2

Each node has several settings that control its behavior. You can access these by clicking on the node.
For example, a Dense layer has two important parameters: the number of units and the activation function.

## 3

The number of units in a layer determines the complexity of the task that the layer can handle. Think of each layer as a
team of workers. The units represent the size or capability of that team. Bigger teams are better equipped to tackle complex tasks.

## 4

However, be cautious! Having too many units is like assembling too large of a team. This can lead to inefficiency, higher
costs, and reduced performance. We’ll discuss this more as we move forward, but for now, set the number of units in the first Dense layer to 128.

## 5

The activation function acts like a team leader, guiding how a layer should process and transform the incoming data.
It plays a crucial role in allowing the network to learn and capture complex patterns. For now, let's set the activation function of the first Dense layer to ReLU.

## 6

Great! ReLU (Rectified Linear Unit) is one of the most widely used activation functions. It helps the network focus on
the most important features by turning off irrelevant information and is highly effective in many machine learning tasks.

## 7

Now, let’s talk about datasets. Think of a dataset like a group of advisors, each with a unique piece of knowledge.
Your network is like a person trying to find the best path through a maze. Before each move, the network asks the advisors (the data) for guidance.

## 8

Today, we’ll use the MNIST dataset. This dataset contains 70,000 images of handwritten digits, each labeled with a digit
from 0 to 9. The goal of our network is to learn to recognize and classify these digits.

## 9

You can find the MNIST dataset in the DATASETS section of the toolbar. Simply drag it onto the canvas to begin using it.

## 10

Great! Now that you have the MNIST dataset, let’s adjust the second Dense layer, also known as the output layer.
The output layer is responsible for generating the final predictions of the network and thus depends on the task at hand.

## 11

Set the number of units in the output layer to 10, as we need to recognize 10 different digits (from 0 to 9).

## 12

Excellent! Finally, set the activation function to Softmax. Softmax is ideal for classification tasks like this one.
It converts the output of the layer into probabilities, showing how likely the network thinks each digit is the correct answer.

## 13

Well done! You've now set up your network to work with the MNIST dataset and configured the key components.
