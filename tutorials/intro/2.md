# LeNet-5

## 1

Welcome to round two! In the previous session, we laid the groundwork for your neural network. Now, we’ll take a closer
look at the settings of the layers and dive into the MNIST dataset.

## 2

Each node has several settings that control its behavior. You can access these by clicking on the node.
For example, a Dense layer has two important parameters: the number of units and the activation function.

## 3

The number of units in a layer determines the complexity of the task that the layer can handle. Think of each layer as a
team of workers. The units represent the size or capability of that team. Bigger teams are better equipped to tackle complex tasks.

## 4

However, be cautious! Having too many units is like assembling too large of a team. This can lead to inefficiency, higher
costs, and reduced performance. We’ll discuss this more as we move forward, but for now, set the number of units in the first Dense layer to 128.

## 5

Next, let’s look at the activation function. Think of it like assigning tasks to the team. The activation function dictates
how the layer processes the data it receives. We'll cover various activation functions in more detail later. For now, set the activation function of the first Dense layer to ReLU.

## 6

Awesome! ReLU is one of the most popular activation functions. It encourages the network to focus on the important
aspects of the data and is highly effective in many cases.

## 7

To understand the impact of the input and the second Dense layer, let’s first talk about datasets. Think of a dataset like
a group of advisors, each with a unique piece of knowledge. Your network is like a person trying to find the best path through a maze.
Before each move, the network asks the advisors (the data) for guidance.

## 8

Today, we’ll use the MNIST dataset. This dataset contains 70,000 images of handwritten digits, each labeled with a digit
from 0 to 9. The goal of our network is to learn to recognize and classify these digits.

## 9

You can find the MNIST dataset in the DATASETS section of the toolbar. Simply drag it onto the canvas to begin using it.

## 10

Great! Now that you have the MNIST dataset, let’s adjust the second Dense layer, also known as the output layer.
The output layer is responsible for generating the final predictions of the network and thus depends on the task at hand.

## 11

Set the number of units in the output layer to 10, as we need to recognize 10 different digits (from 0 to 9).

## 12

Excellent! Finally, set the activation function to Softmax. Softmax is ideal for classification tasks like this one.
It converts the output of the layer into probabilities, showing how likely the network thinks each digit is the correct answer.

## 13

Well done! You've now set up your network to work with the MNIST dataset and configured the key components. In the next
session, we'll move forward by incorporating the dataset into the model and adding two more layers to enhance its ability to classify the images effectively.
