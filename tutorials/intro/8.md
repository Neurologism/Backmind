# LeNet-5

## 1

GPT! GPT! GPT! Generative Pretrained Transformer! ............. We'll talk more about
this exciting topic in the future, I promise! For now, let's focus on optimizing your model, shall we?

## 2

To help your model learn faster and more effectively, let’s add a normalization layer. Disconnect the Input layer from
the first Dense layer, and instead, connect the Input layer to a Normalization layer.

## 3

Let’s increase your model’s capacity by adding more Dense layers. First, add a Dense layer with 1024 units. Then, add
another Dense layer with 512 units. These layers will give your model more power to learn complex patterns.

## 4

For both of the new Dense layers, select the Tanh activation function.

## 5

Now, let’s structure the model as follows: Input -> Normalization -> Dense (1024 units) -> Dense (512 units) -> Dense (32 units)
-> Output(1 unit). Take a moment to set this up.

## 6

All set? Hit the Train button and let’s see how your model performs with these changes.

## 7

Amazing work! Your model’s performance has improved significantly. Previously, the Mean Absolute Error (MAE) was around 4.5,
but now it’s down to 2.5. Great progress!

## 8

We can do even better. In Tutorial 5, we discussed how the number of epochs can affect performance.
Let’s increase the number of epochs to 100. You can set this in the Fit node.

## 9

Add a Line Chart node to monitor your progress. Set the y label to MAE and connect it to the Fit node. Now, hit Train and observe the model’s performance.

## 10

Wow! Your model’s MAE is now down to 0.2. That’s a huge improvement! However, you might have noticed that training took
a bit longer this time. This is a trade-off when increasing the number of epochs.

## 11

There’s a simple trick to reduce training time: increase the batch size. Remember our example with the maze and advisors?
The batch size is like the number of advisors you consult before making each move. Let’s set the batch size to 128 in the Wine Quality dataset and train the model again.

## 12

Good news! Training time was significantly reduced. The MAE increased slightly to around 0.3, but that’s still a strong
performance. Why does this happen?

## 13

Think of it this way: One epoch means consulting all advisors (data) once. With a larger batch size, you consult fewer
groups of advisors but get more precise feedback per group. Smaller batch sizes mean more frequent feedback but less precise per update.

## 14

Choosing the right batch size depends on your dataset and model complexity. Experimentation is key to finding the optimal value.

## 15

Fantastic work! Keep experimenting, and keep improving.
